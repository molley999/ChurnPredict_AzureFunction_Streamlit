{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c423a85-79d1-49d5-9129-fbed41e23969",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "import libs"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, when, min, max\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as td\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "from hyperopt import STATUS_OK, fmin, tpe, hp, Trials\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eae77645-5b2a-4a72-9ee1-e138625c0a80",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "functions defined"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(df, column_name):\n",
    "    distinct_values = [row[column_name] for row in df.select(column_name).distinct().collect()]\n",
    "    \n",
    "    for value in distinct_values:\n",
    "        df = df.withColumn(f\"{column_name}_{value}\", when(col(column_name) == value, 1).otherwise(0))\n",
    "    \n",
    "    df = df.drop(column_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def normalize_column(df, column_name):\n",
    "    min_value = df.agg(min(col(column_name))).first()[0]\n",
    "    max_value = df.agg(max(col(column_name))).first()[0]\n",
    "    return df.withColumn(column_name, (col(column_name) - min_value) / (max_value - min_value))\n",
    "\n",
    "\n",
    "# Define the neural network architecture\n",
    "class ChurnNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ChurnNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 15)\n",
    "        self.fc2 = nn.Linear(15, 8)\n",
    "        self.fc3 = nn.Linear(8, 2)\n",
    "   \n",
    "    def forward(self, x):\n",
    "        fc1_output = torch.relu(self.fc1(x))\n",
    "        fc2_output = torch.relu(self.fc2(fc1_output))\n",
    "        y = F.log_softmax(self.fc3(fc2_output).float(), dim=1)\n",
    "        return y  \n",
    "    \n",
    "\n",
    "def train(model, data_loader, optimizer, loss_criteria):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch, tensor in enumerate(data_loader):\n",
    "        data, target = tensor\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = loss_criteria(out, target)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_loss = train_loss / (batch+1)\n",
    "    # print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def test(model, data_loader, loss_criteria):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        batch_count = 0\n",
    "        for batch, tensor in enumerate(data_loader):\n",
    "            batch_count += 1\n",
    "            data, target = tensor\n",
    "            out = model(data)\n",
    "            test_loss += loss_criteria(out, target).item()\n",
    "            _, predicted = torch.max(out.data, 1)\n",
    "            correct += torch.sum(target==predicted).item()\n",
    "    avg_loss = test_loss/batch_count\n",
    "    # print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        # avg_loss, correct, len(data_loader.dataset),\n",
    "        # 100. * correct / len(data_loader.dataset)))\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "671449d6-cbcb-40cb-890d-6d3abfc48fbd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "encapsulate the process"
    }
   },
   "outputs": [],
   "source": [
    "class ChurnPredictionPipeline:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def reset_df(self):\n",
    "        self.df = spark.sql(\"SELECT * FROM hive_metastore.default.telco_customer_churn\")\n",
    "\n",
    "    def data_cleasing(self):\n",
    "        self.df = self.df.na.drop()\n",
    "        self.df = self.df.withColumn(\"TotalCharges\", col(\"TotalCharges\").cast(DoubleType()))\n",
    "        self.df = self.df.withColumn(\"Churn\", when(col(\"Churn\") == \"Yes\", 1).otherwise(0))\n",
    "        self.df = self.df.dropDuplicates(['customerID', \"TotalCharges\"])\n",
    "        df_minority = self.df.filter((col(\"SeniorCitizen\") == 1) & (col(\"PhoneService\") == \"No\"))\n",
    "        df_minority = df_minority.sample(withReplacement=True, fraction=15.0, seed=42)\n",
    "        self.df = df_minority.union(self.df.filter((col(\"SeniorCitizen\") == 0) | (col(\"PhoneService\") == \"Yes\")))\n",
    "        self.df = self.df.na.drop()\n",
    "\n",
    "    def feature_engineering(self):\n",
    "        self.df = self.df.drop(\"customerID\")\n",
    "        categorical_columns = [\"gender\",'SeniorCitizen', \"Partner\", \"Dependents\", \"PhoneService\", \"MultipleLines\", \"InternetService\", \"OnlineSecurity\", \"OnlineBackup\", \"DeviceProtection\", \"TechSupport\", \"StreamingTV\", \"StreamingMovies\", \"Contract\", \"PaperlessBilling\", \"PaymentMethod\"]\n",
    "        numeric_columns = [\"tenure\", \"MonthlyCharges\", \"TotalCharges\"]\n",
    "        for col_name in categorical_columns:\n",
    "            self.df = one_hot_encode(self.df, col_name)\n",
    "        for col_name in numeric_columns:\n",
    "            self.df = normalize_column(self.df, col_name)\n",
    "    \n",
    "    def modelling_split_data(self):\n",
    "        features = [col for col in self.df.columns if col != \"Churn\"]\n",
    "        label = \"Churn\"\n",
    "        x_train, x_test, y_train, y_test = train_test_split(\n",
    "            self.df.toPandas()[features].values,\n",
    "            self.df.toPandas()[label].values,\n",
    "            test_size=0.30,\n",
    "            random_state=0)\n",
    "        print ('\\nTraining Set: %d rows, Test Set: %d rows \\n' % (len(x_train), len(x_test)))\n",
    "        torch.manual_seed(0)\n",
    "        return x_train, x_test, y_train, y_test\n",
    "    \n",
    "    def modelling_create_dataloaders(self, x_train, x_test, y_train, y_test, batch_size=20):\n",
    "        train_x = torch.Tensor(x_train).float()\n",
    "        train_y = torch.Tensor(y_train).long()\n",
    "        train_ds = td.TensorDataset(train_x,train_y)\n",
    "        train_loader = td.DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "        test_x = torch.Tensor(x_test).float()\n",
    "        test_y = torch.Tensor(y_test).long()\n",
    "        test_ds = td.TensorDataset(test_x,test_y)\n",
    "        test_loader = td.DataLoader(test_ds, batch_size=batch_size,\n",
    "        shuffle=False, num_workers=1)\n",
    "        return train_loader, test_loader, train_x.shape[1]\n",
    "\n",
    "    def objective(self, params):\n",
    "        epochs = int(params['epochs'])\n",
    "        learning_rate = params['learning_rate']\n",
    "        batch_size = int(params['batch_size'])\n",
    "\n",
    "        self.reset_df()\n",
    "        self.data_cleasing()\n",
    "        self.feature_engineering()\n",
    "        x_train, x_test, y_train, y_test = self.modelling_split_data()\n",
    "        train_loader, test_loader, input_dim = self.modelling_create_dataloaders(x_train, x_test, y_train, y_test, batch_size=batch_size)\n",
    "        model = ChurnNet(input_dim)\n",
    "        print(f\"Below is the defined architecture: \\n{model}\\n\")\n",
    "\n",
    "        loss_criteria = nn.CrossEntropyLoss() # loss criteria: CrossEntropyLoss for multi-class classification\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # optimizer to adjust weights and reduce loss\n",
    "        optimizer.zero_grad()\n",
    "        epoch_nums = []\n",
    "        training_loss = []\n",
    "        validation_loss = []\n",
    "        with mlflow.start_run() as run:\n",
    "            mlflow.log_param(\"epochs\", epochs)\n",
    "            mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "            mlflow.log_param(\"batch_size\", batch_size)\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                train_loss = train(model, train_loader, optimizer, loss_criteria)\n",
    "                test_loss = test(model, test_loader, loss_criteria)\n",
    "\n",
    "                epoch_nums.append(epoch)\n",
    "                training_loss.append(train_loss)\n",
    "                validation_loss.append(test_loss)\n",
    "\n",
    "                mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "                mlflow.log_metric(\"test_loss\", test_loss, step=epoch)\n",
    "                \n",
    "            signature = infer_signature(pd.DataFrame(x_test), model(torch.tensor(x_test).float()).detach().numpy())\n",
    "            mlflow.pytorch.log_model(model, \"model\", signature=signature)\n",
    "        return {'loss': test_loss, \n",
    "                'status': STATUS_OK}\n",
    "\n",
    "    def hyperparameter_tuning(self, max_evals=3):\n",
    "        search_space = {\n",
    "        'epochs': hp.quniform('epochs', 5, 10, 5),\n",
    "        'learning_rate': hp.loguniform('learning_rate', -5, -3),\n",
    "        'batch_size': hp.quniform('batch_size', 5, 30, 5)}\n",
    "        trials = Trials()\n",
    "        argmin = fmin(\n",
    "            fn=self.objective,\n",
    "            space=search_space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=max_evals,\n",
    "            trials=trials)\n",
    "        \n",
    "        print(\"Best param values: \", argmin)\n",
    "        return argmin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "239f2d33-6082-49ac-97a3-44b1835d242a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "execute model training and testing"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/3 [00:00<?, ?trial/s, best loss=?]\r                                                     \r\nTraining Set: 5968 rows, Test Set: 2559 rows \n\n\r  0%|          | 0/3 [00:14<?, ?trial/s, best loss=?]\r                                                     \rBelow is the defined architecture: \nChurnNet(\n  (fc1): Linear(in_features=46, out_features=15, bias=True)\n  (fc2): Linear(in_features=15, out_features=8, bias=True)\n  (fc3): Linear(in_features=8, out_features=2, bias=True)\n)\n\n\r  0%|          | 0/3 [00:14<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfb3cf46785e4c10aa2b5793e6475fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 33%|███▎      | 1/3 [01:20<02:40, 80.01s/trial, best loss: 0.3987602092784073]\r                                                                               \r\nTraining Set: 5968 rows, Test Set: 2559 rows \n\n\r 33%|███▎      | 1/3 [01:35<02:40, 80.01s/trial, best loss: 0.3987602092784073]\r                                                                               \rBelow is the defined architecture: \nChurnNet(\n  (fc1): Linear(in_features=46, out_features=15, bias=True)\n  (fc2): Linear(in_features=15, out_features=8, bias=True)\n  (fc3): Linear(in_features=8, out_features=2, bias=True)\n)\n\n\r 33%|███▎      | 1/3 [01:35<02:40, 80.01s/trial, best loss: 0.3987602092784073]"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4275df18fa84456aa28b0b3027ced234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 67%|██████▋   | 2/3 [02:14<01:04, 64.83s/trial, best loss: 0.3987602092784073]\r                                                                               \r\nTraining Set: 5968 rows, Test Set: 2559 rows \n\n\r 67%|██████▋   | 2/3 [02:28<01:04, 64.83s/trial, best loss: 0.3987602092784073]\r                                                                               \rBelow is the defined architecture: \nChurnNet(\n  (fc1): Linear(in_features=46, out_features=15, bias=True)\n  (fc2): Linear(in_features=15, out_features=8, bias=True)\n  (fc3): Linear(in_features=8, out_features=2, bias=True)\n)\n\n\r 67%|██████▋   | 2/3 [02:28<01:04, 64.83s/trial, best loss: 0.3987602092784073]"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7939a0bfd08c415d82c0ec2da9d31224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r100%|██████████| 3/3 [02:55<00:00, 54.04s/trial, best loss: 0.3987602092784073]\r100%|██████████| 3/3 [02:55<00:00, 58.47s/trial, best loss: 0.3987602092784073]\nBest param values:  {'batch_size': 5.0, 'epochs': 10.0, 'learning_rate': 0.008313049444266815}\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"SELECT * FROM hive_metastore.default.telco_customer_churn\")\n",
    "pipeline = ChurnPredictionPipeline(df)\n",
    "Best_params = pipeline.hyperparameter_tuning(max_evals=3)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "churn_predict_v3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
